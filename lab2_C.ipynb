{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bec5e71",
   "metadata": {},
   "source": [
    "# Level C instructions:\n",
    "\n",
    "### Some steps to get familiar with the data:\n",
    "\n",
    "- Have a look at the rgb images in the folder \"ARKitScenesData/47333473/47333473_frames/lowres_wide/\" to get an impression of the scene that we are working with\n",
    "- To start, run the notebook which will show ground truth visualization, example detection, and full pipeline results\n",
    "\n",
    "## Level C\n",
    "\n",
    "The task is to generate 3D bounding boxes that mark the estimated location of objects in the environment.\n",
    "This time using OWLV2 detections with SAM refinement.\n",
    "\n",
    "Required to pass the level:\n",
    "- Functional pipeline with visualization of the estimated 3D bounding boxes\n",
    "- mIOU score > 0.25 of the bounding box estimates when comparing to ground-truth (For classes: \"bed\", \"sofa\", \"chair\", \"table\", \"shelf\")\n",
    "- Understand and explain the code flow and steps required for the complete pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e4a38",
   "metadata": {},
   "source": [
    "## 1. Dependencies and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install torch==2.4.0+cu121 torchvision==0.19.0+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers==4.44.0 huggingface-hub==0.24.0 pillow numpy opencv-python open3d ipympl rerun-sdk[notebook]==0.24.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0eba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Import lab utility functions\n",
    "from lab_utils.data_utils import get_frame_list, load_camera_poses, validate_and_align_frame_data\n",
    "from lab_utils.ground_truth import load_ground_truth_data\n",
    "from lab_utils.tsdf_utils import build_tsdf_point_cloud\n",
    "from lab_utils.viz_eval import visualize_3d_scene_bbox_results, evaluate_level_results\n",
    "from lab_utils.model_loaders import load_owlv2_model, load_sam_model\n",
    "from lab_utils.level_specific_viz import visualize_level_c_example\n",
    "from lab_utils.batch_processing_utils import process_frames_with_sam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba0ce0",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100b267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Scene Configuration\n",
    "    SCENE_ID = \"47333473\"\n",
    "    BASE_PATH = f\"ARKitScenesData/{SCENE_ID}/{SCENE_ID}_frames\"\n",
    "    RGB_PATH = os.path.join(BASE_PATH, \"lowres_wide\")\n",
    "    DEPTH_PATH = os.path.join(BASE_PATH, \"lowres_depth\")\n",
    "    INTRINSICS_PATH = os.path.join(BASE_PATH, \"lowres_wide_intrinsics\")\n",
    "    TRAJ_FILE_PATH = os.path.join(BASE_PATH, \"lowres_wide.traj\")\n",
    "    \n",
    "    # Detection Classes\n",
    "    OBJECT_CLASSES = [\"bed\", \"chair\", \"sofa\", \"table\", \"shelf\"]\n",
    "    \n",
    "    # Level C Configuration\n",
    "    LEVEL_C_CONFIG = {\n",
    "        'frame_skip': 3,\n",
    "        'max_frames': 1000,\n",
    "        'detection_threshold': 0.4,       #[modify]\n",
    "        'sam_confidence_threshold': 0.3,  #[modify]\n",
    "        'overlap_threshold': 0.5,         #[modify]\n",
    "        'min_observations': 6,            #[modify]\n",
    "        'sam_model_size': 'base',\n",
    "        'max_points_per_object': 2000,\n",
    "        'example_viz_index': 65,\n",
    "        'sam_batch_size': 5,\n",
    "        'required_miou': 0.25,            #[Do not modify]\n",
    "    }\n",
    "    \n",
    "    # TSDF Configuration\n",
    "    TSDF_CONFIG = {\n",
    "        'frame_skip': 3, 'depth_scale': 1000.0, 'depth_trunc': 7.0,\n",
    "        'voxel_size': 0.04, 'batch_size': 20, 'max_frames': 1000,\n",
    "        'volume_length': 30.0, 'resolution': 512,\n",
    "    }\n",
    "\n",
    "    # GT and Display Configuration\n",
    "    GT_CONFIG = {\n",
    "        'allowed_classes': None, 'mesh_downsample_points': 75000,\n",
    "        'show_mesh': True, 'show_annotations': True\n",
    "    }\n",
    "    RERUN_WIDTH, RERUN_HEIGHT = 1200, 700\n",
    "\n",
    "def validate_config(config: Config) -> None:\n",
    "    \"\"\"Validate and initialize configuration parameters.\"\"\"\n",
    "    config.GT_CONFIG['allowed_classes'] = config.OBJECT_CLASSES\n",
    "    print(f\"Configuration validated.\")\n",
    "\n",
    "# Create and validate config\n",
    "config = Config()\n",
    "validate_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0jvnufc0i31",
   "metadata": {},
   "source": [
    "## 3. Scene Understanding - Ground Truth Visualization\n",
    "\n",
    "Let's start by visualizing the ground truth data to understand what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4as7q8na7tx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize ground truth to understand the scene\n",
    "print(\"Loading ground truth data to understand our scene...\")\n",
    "\n",
    "gt_annotations, gt_mesh = load_ground_truth_data(\n",
    "    config.SCENE_ID, \n",
    "    config.BASE_PATH,\n",
    "    config.GT_CONFIG\n",
    ")\n",
    "\n",
    "if gt_annotations:\n",
    "    print(f\"✓ Loaded {len(gt_annotations)} ground truth annotations\")\n",
    "    class_counts = {}\n",
    "    for ann in gt_annotations:\n",
    "        class_counts[ann['label']] = class_counts.get(ann['label'], 0) + 1\n",
    "    print(f\"Objects in scene: {class_counts}\")\n",
    "\n",
    "if gt_mesh:\n",
    "    print(f\"✓ Loaded ground truth mesh with {len(gt_mesh.points)} points\")\n",
    "\n",
    "# Visualize the ground truth scene\n",
    "visualize_3d_scene_bbox_results(\n",
    "    point_cloud=None,\n",
    "    detections_3d=None,\n",
    "    gt_annotations=gt_annotations,\n",
    "    gt_mesh=gt_mesh,\n",
    "    show_ground_truth=True,\n",
    "    show_gt_mesh=True,\n",
    "    show_object_pointclouds=False,\n",
    "    title=f\"Ground Truth Scene {config.SCENE_ID} - What We Want to Detect\",\n",
    "    config=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b162a8e",
   "metadata": {},
   "source": [
    "## 4. Execution Functions\n",
    "\n",
    "These functions orchestrate the different parts of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6be7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ground_truth_visualization(config: Config) -> None:\n",
    "    \"\"\"Execute ground truth visualization.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"GROUND TRUTH VISUALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    gt_annotations, gt_mesh = load_ground_truth_data(\n",
    "        config.SCENE_ID, \n",
    "        config.BASE_PATH,\n",
    "        config.GT_CONFIG\n",
    "    )\n",
    "    \n",
    "    if gt_annotations:\n",
    "        print(f\"Loaded {len(gt_annotations)} ground truth annotations\")\n",
    "        class_counts = {}\n",
    "        for ann in gt_annotations:\n",
    "            class_counts[ann['label']] = class_counts.get(ann['label'], 0) + 1\n",
    "        print(f\"GT objects by class: {class_counts}\")\n",
    "    \n",
    "    if gt_mesh:\n",
    "        print(f\"Loaded ground truth mesh with {len(gt_mesh.points)} points\")\n",
    "    \n",
    "    visualize_3d_scene_bbox_results(\n",
    "        point_cloud=None,\n",
    "        detections_3d=None,\n",
    "        gt_annotations=gt_annotations,\n",
    "        gt_mesh=gt_mesh,\n",
    "        show_ground_truth=True,\n",
    "        show_gt_mesh=True,\n",
    "        show_object_pointclouds=False,\n",
    "        title=f\"Ground Truth Only - Scene {config.SCENE_ID}\",\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    print(\"Ground truth visualization complete!\")\n",
    "\n",
    "\n",
    "def run_example_visualization(config: Config) -> Dict:\n",
    "    \"\"\"Execute example visualization.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXAMPLE VISUALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    example_results = visualize_level_c_example(\n",
    "        config, \n",
    "        frame_index=config.LEVEL_C_CONFIG['example_viz_index']\n",
    "    )\n",
    "    \n",
    "    print(\"Example visualization complete!\")\n",
    "    return example_results\n",
    "\n",
    "\n",
    "def run_full_pipeline(config: Config) -> Dict:\n",
    "    \"\"\"Execute the complete SAM-enhanced 3D scene analysis pipeline.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FULL PIPELINE EXECUTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    owl_processor, owl_model, device = load_owlv2_model()\n",
    "    sam_model, sam_processor, _ = load_sam_model(\n",
    "        model_size=config.LEVEL_C_CONFIG['sam_model_size'], \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    camera_poses = load_camera_poses(config.TRAJ_FILE_PATH)\n",
    "    frames_metadata = get_frame_list(config.RGB_PATH, config.LEVEL_C_CONFIG['frame_skip'])\n",
    "    aligned_frames = validate_and_align_frame_data(\n",
    "        frames_metadata, camera_poses, config.RGB_PATH, \n",
    "        config.DEPTH_PATH, config.INTRINSICS_PATH, timestamp_tolerance=0.1\n",
    "    )\n",
    "    \n",
    "    if not aligned_frames:\n",
    "        print(\"ERROR: No aligned frames found! Check data paths.\")\n",
    "        return {'detections_3d': [], 'frame_results': {}, 'statistics': {}}\n",
    "    \n",
    "    frames_for_detection = aligned_frames[:config.LEVEL_C_CONFIG['max_frames']]\n",
    "    raw_pointclouds, frame_results, detection_stats = process_frames_with_sam(\n",
    "        frames_for_detection, config, owl_processor, owl_model, \n",
    "        sam_processor, sam_model, device\n",
    "    )\n",
    "    \n",
    "    fused_objects = fuse_object_pointclouds(\n",
    "        raw_pointclouds,\n",
    "        overlap_threshold=config.LEVEL_C_CONFIG['overlap_threshold'],\n",
    "        min_observations=config.LEVEL_C_CONFIG['min_observations']\n",
    "    ) if raw_pointclouds else []\n",
    "    \n",
    "    final_detections = generate_final_detections(fused_objects)\n",
    "    \n",
    "    print(f\"Pipeline: {len(frames_for_detection)} frames → {len(raw_pointclouds)} pointclouds → {len(fused_objects)} fused → {len(final_detections)} final\")\n",
    "    \n",
    "    tsdf_point_cloud = build_tsdf_point_cloud(config, max_frames_for_mapping=596, use_cached=True)\n",
    "    gt_annotations, gt_mesh = load_ground_truth_data(config.SCENE_ID, config.BASE_PATH, config.GT_CONFIG)\n",
    "    eval_results = evaluate_level_results(final_detections, gt_annotations, \"Level C (OwlV2 + SAM)\", required_miou=config.LEVEL_C_CONFIG['required_miou'])\n",
    "    \n",
    "    print(f\"Results: {'✓ PASSED' if eval_results['passed'] else '✗ FAILED'} | \"\n",
    "          f\"mIoU: {eval_results['mean_iou']:.3f} | Detections: {eval_results['num_detections']}\")\n",
    "    \n",
    "    if final_detections or tsdf_point_cloud:\n",
    "        visualize_3d_scene_bbox_results(\n",
    "            point_cloud=tsdf_point_cloud, detections_3d=final_detections,\n",
    "            gt_annotations=gt_annotations, gt_mesh=None, \n",
    "            show_ground_truth=True, show_gt_mesh=False,\n",
    "            show_object_pointclouds=True,\n",
    "            title=f\"Level C: SAM-Enhanced Detection - Scene {config.SCENE_ID}\",\n",
    "            config=config\n",
    "        )\n",
    "    \n",
    "    print(\"Full pipeline complete!\")\n",
    "    return {\n",
    "        'detections_3d': final_detections,\n",
    "        'raw_pointclouds': raw_pointclouds,\n",
    "        'fused_objects': fused_objects,\n",
    "        'frame_results': frame_results,\n",
    "        'statistics': {\n",
    "            **detection_stats,\n",
    "            'total_fused_objects': len(fused_objects),\n",
    "            'final_detections': len(final_detections),\n",
    "            'detection_classes': list(detection_stats['detection_classes']),\n",
    "            'alignment_success_rate': len(aligned_frames) / len(frames_metadata) * 100 if frames_metadata else 0,\n",
    "            'fusion_ratio': len(final_detections) / len(raw_pointclouds) if raw_pointclouds else 0\n",
    "        },\n",
    "        'evaluation': eval_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c733f",
   "metadata": {},
   "source": [
    "## 6. 3D Processing Implementation\n",
    "Implement the TODOs below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a614a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_with_sam_bbox(image: Image.Image,\n",
    "                          bbox: List[float],\n",
    "                          sam_model,\n",
    "                          sam_processor,\n",
    "                          device: str,\n",
    "                          confidence_threshold: float = 0.01) -> Optional[np.ndarray]:\n",
    "    \"\"\"Generate segmentation mask using SAM with bounding box prompt.\"\"\"\n",
    "    try:\n",
    "        # Convert bbox to SAM format: [[x1, y1, x2, y2]]\n",
    "        input_boxes = [bbox]\n",
    "        \n",
    "        # Prepare inputs with bounding box prompt\n",
    "        inputs = sam_processor(\n",
    "            images=image,\n",
    "            input_boxes=[input_boxes],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                 for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate mask\n",
    "        with torch.no_grad():\n",
    "            outputs = sam_model(**inputs)\n",
    "        \n",
    "        # Process outputs\n",
    "        masks = sam_processor.image_processor.post_process_masks(\n",
    "            outputs.pred_masks.cpu(),\n",
    "            inputs[\"original_sizes\"].cpu(),\n",
    "            inputs[\"reshaped_input_sizes\"].cpu()\n",
    "        )\n",
    "        \n",
    "        # Extract best mask\n",
    "        batch_masks = masks[0]\n",
    "        if len(batch_masks) == 0:\n",
    "            return None\n",
    "            \n",
    "        box_masks = batch_masks[0]\n",
    "        if len(box_masks) == 0:\n",
    "            return None\n",
    "        \n",
    "        # ==================== STUDENT TASK ====================\n",
    "        # TODO: Select the best mask based on IoU scores\n",
    "        # \n",
    "        # SAM generates multiple mask proposals for each bounding box.\n",
    "        # Each mask has an associated IoU (Intersection over Union) score\n",
    "        # indicating the model's confidence in that mask.\n",
    "        #\n",
    "        # Your task:\n",
    "        # 1. Check if IoU scores are available in the outputs\n",
    "        # 2. If available, find the index of the mask with the highest IoU score\n",
    "        # 3. Extract the best score and check if it meets the confidence threshold\n",
    "        # 4. If below threshold, return None (mask quality too low)\n",
    "        # 5. Handle any potential errors gracefully\n",
    "        # 6. Ensure best_mask_idx is valid for the box_masks array\n",
    "        #\n",
    "        # Available variables:\n",
    "        # - outputs: SAM model outputs (check for 'iou_scores' attribute)\n",
    "        # - box_masks: list of mask proposals\n",
    "        # - confidence_threshold: minimum acceptable IoU score\n",
    "        #\n",
    "        # Variables to set:\n",
    "        # - best_mask_idx: index of the best mask (default to 0)\n",
    "        # - mask: the selected mask from box_masks\n",
    "        \n",
    "        # Initialize default mask index\n",
    "        # best_mask_idx = 0\n",
    "        \n",
    "        # TODO: Check if IoU scores exist and select best mask\n",
    "        # if hasattr(outputs, 'iou_scores') and outputs.iou_scores is not None:\n",
    "        #     try:\n",
    "        #         # Extract IoU scores for the first image and first box\n",
    "        #         # iou_scores = ...  # Placeholder\n",
    "        #         \n",
    "        #         if len(iou_scores) > 0:\n",
    "        #             # Find index of highest scoring mask\n",
    "        #             # best_mask_idx = ...  # Placeholder\n",
    "        #             # best_score = ...  # Placeholder\n",
    "        #             \n",
    "        #             # Check if score meets threshold\n",
    "        #             # if best_score < confidence_threshold:\n",
    "        #             #     return None\n",
    "        #     except (IndexError, RuntimeError):\n",
    "        #         pass  # Use default mask if error occurs\n",
    "        #             \n",
    "        # if best_mask_idx >= len(box_masks):\n",
    "        #     best_mask_idx = 0\n",
    "        # \n",
    "        # mask = ...  # Placeholder - select mask using best_mask_idx\n",
    "        \n",
    "        best_mask_idx = 0  # Placeholder\n",
    "        mask = box_masks[0]  # Placeholder - replace with proper selection\n",
    "        # ==================== END STUDENT TASK ====================\n",
    "        \n",
    "        # Convert to binary numpy array\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask_np = mask.cpu().numpy().astype(bool)\n",
    "        else:\n",
    "            mask_np = np.array(mask).astype(bool)\n",
    "        \n",
    "        return mask_np\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SAM segmentation failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b42b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects_in_frame(image_path: str, \n",
    "                           text_queries: List[str],\n",
    "                           processor, \n",
    "                           model, \n",
    "                           device: str,\n",
    "                           threshold: float = 0.1) -> Tuple[List[Dict], Image.Image]:\n",
    "    \"\"\"Run OwlV2 object detection.\"\"\"\n",
    "    \n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    orig_width, orig_height = image.size\n",
    "    \n",
    "    inputs = processor(text=text_queries, images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Use standard post-processing\n",
    "    target_sizes = torch.tensor([[orig_height, orig_width]]).to(device)\n",
    "    \n",
    "    # Process outputs\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs=outputs, target_sizes=target_sizes, threshold=threshold\n",
    "    )[0]\n",
    "\n",
    "    # Extract and convert to numpy\n",
    "    boxes = results[\"boxes\"].cpu().numpy()\n",
    "    scores = results[\"scores\"].cpu().numpy()\n",
    "    labels = results[\"labels\"].cpu().numpy()\n",
    "    \n",
    "    # Create detection list\n",
    "    detections = []\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        # Clip to image bounds\n",
    "        bbox = [\n",
    "            max(0, min(box[0], orig_width)),\n",
    "            max(0, min(box[1], orig_height)),\n",
    "            max(0, min(box[2], orig_width)),\n",
    "            max(0, min(box[3], orig_height))\n",
    "        ]\n",
    "        \n",
    "        detections.append({\n",
    "            'bbox': bbox,\n",
    "            'score': float(score),\n",
    "            'label': text_queries[label],\n",
    "            'label_id': int(label)\n",
    "        })\n",
    "    \n",
    "    return detections, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r55o1a2db5",
   "metadata": {},
   "source": [
    "Visualize the detector with SAM refinement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u1n6pr5a0i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run example visualization to see SAM-enhanced detection process on a single frame\n",
    "print(\"Running example SAM-enhanced detection on a single frame...\")\n",
    "\n",
    "example_results = visualize_level_c_example(\n",
    "    config, \n",
    "    frame_index=config.LEVEL_C_CONFIG['example_viz_index']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae309f",
   "metadata": {},
   "source": [
    "## 5. 3D Processing Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_with_sam_bbox(image: Image.Image,\n",
    "                          bbox: List[float],\n",
    "                          sam_model,\n",
    "                          sam_processor,\n",
    "                          device: str,\n",
    "                          confidence_threshold: float = 0.01) -> Optional[np.ndarray]:\n",
    "    \"\"\"Generate segmentation mask using SAM with bounding box prompt.\"\"\"\n",
    "    try:\n",
    "        # Convert bbox to SAM format: [[x1, y1, x2, y2]]\n",
    "        input_boxes = [bbox]\n",
    "        \n",
    "        # Prepare inputs with bounding box prompt\n",
    "        inputs = sam_processor(\n",
    "            images=image,\n",
    "            input_boxes=[input_boxes],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                 for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate mask\n",
    "        with torch.no_grad():\n",
    "            outputs = sam_model(**inputs)\n",
    "        \n",
    "        # Process outputs\n",
    "        masks = sam_processor.image_processor.post_process_masks(\n",
    "            outputs.pred_masks.cpu(),\n",
    "            inputs[\"original_sizes\"].cpu(),\n",
    "            inputs[\"reshaped_input_sizes\"].cpu()\n",
    "        )\n",
    "        \n",
    "        # Extract best mask\n",
    "        batch_masks = masks[0]\n",
    "        if len(batch_masks) == 0:\n",
    "            return None\n",
    "            \n",
    "        box_masks = batch_masks[0]\n",
    "        if len(box_masks) == 0:\n",
    "            return None\n",
    "        \n",
    "        # ==================== STUDENT TASK 1 ====================\n",
    "        # TODO: Select the best mask based on IoU scores\n",
    "        # \n",
    "        # SAM generates multiple mask proposals for each bounding box.\n",
    "        # Each mask has an associated IoU score indicating confidence.\n",
    "        #\n",
    "        # Your task:\n",
    "        # 1. Check if IoU scores exist in outputs using hasattr()\n",
    "        # 2. Extract scores from outputs.iou_scores[0][0] \n",
    "        # 3. Find the index of the highest score using torch.argmax()\n",
    "        # 4. Get the score value and check against confidence_threshold\n",
    "        # 5. Return None if below threshold\n",
    "        # 6. Select the mask at best_mask_idx from box_masks\n",
    "        #\n",
    "        # Variables provided:\n",
    "        # - outputs: SAM model outputs (check for 'iou_scores' attribute)\n",
    "        # - box_masks: list of mask proposals\n",
    "        # - confidence_threshold: minimum acceptable IoU score\n",
    "        \n",
    "        best_mask_idx = 0  # Default to first mask\n",
    "        \n",
    "        # TODO: Your code here (approximately 8-10 lines)\n",
    "        # if hasattr(outputs, 'iou_scores') and outputs.iou_scores is not None:\n",
    "        #     try:\n",
    "        #         iou_scores = outputs.iou_scores[0][0]\n",
    "        #         if len(iou_scores) > 0:\n",
    "        #             best_mask_idx = ...  # TODO: find index of max score\n",
    "        #             best_score = ...     # TODO: get the score value\n",
    "        #             if best_score < confidence_threshold:\n",
    "        #                 return None\n",
    "        #     except (IndexError, RuntimeError):\n",
    "        #         pass\n",
    "        \n",
    "        # Validate index and select mask\n",
    "        if best_mask_idx >= len(box_masks):\n",
    "            best_mask_idx = 0\n",
    "        \n",
    "        mask = box_masks[best_mask_idx]  # TODO: Replace with proper selection\n",
    "        # ==================== END STUDENT TASK 1 ====================\n",
    "        \n",
    "        # Convert to binary numpy array\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask_np = mask.cpu().numpy().astype(bool)\n",
    "        else:\n",
    "            mask_np = np.array(mask).astype(bool)\n",
    "        \n",
    "        return mask_np\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SAM segmentation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def segment_to_3d_pointcloud(mask: np.ndarray,\n",
    "                             depth_image: np.ndarray,\n",
    "                             rgb_image: np.ndarray,\n",
    "                             camera_intrinsics: np.ndarray,\n",
    "                             camera_pose: np.ndarray,\n",
    "                             depth_scale: float = 1000.0,\n",
    "                             max_points: int = 5000,\n",
    "                             min_depth: float = 0.1,\n",
    "                             max_depth: float = 10.0) -> Optional[Dict]:\n",
    "    \"\"\"Convert segmented region to 3D point cloud.\"\"\"\n",
    "    # Get mask indices\n",
    "    mask_indices = np.where(mask)\n",
    "    \n",
    "    if len(mask_indices[0]) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Extract depth values for masked region\n",
    "    depths = depth_image[mask_indices] / depth_scale\n",
    "    \n",
    "    # Filter valid depths\n",
    "    valid_mask = (depths > min_depth) & (depths < max_depth)\n",
    "    if not np.any(valid_mask):\n",
    "        return None\n",
    "    \n",
    "    # Get valid coordinates and depths\n",
    "    v_coords = mask_indices[0][valid_mask]\n",
    "    u_coords = mask_indices[1][valid_mask]\n",
    "    valid_depths = depths[valid_mask]\n",
    "    \n",
    "    # Subsample if too many points\n",
    "    num_points = len(v_coords)\n",
    "    if num_points > max_points:\n",
    "        indices = np.random.choice(num_points, max_points, replace=False)\n",
    "        v_coords = v_coords[indices]\n",
    "        u_coords = u_coords[indices]\n",
    "        valid_depths = valid_depths[indices]\n",
    "    \n",
    "    # Get camera parameters\n",
    "    fx, fy = camera_intrinsics[0, 0], camera_intrinsics[1, 1]\n",
    "    cx, cy = camera_intrinsics[0, 2], camera_intrinsics[1, 2]\n",
    "    \n",
    "    # Project to 3D camera coordinates\n",
    "    x_cam = (u_coords - cx) * valid_depths / fx\n",
    "    y_cam = (v_coords - cy) * valid_depths / fy\n",
    "    z_cam = valid_depths\n",
    "    \n",
    "    # Stack into points\n",
    "    points_cam = np.stack([x_cam, y_cam, z_cam], axis=-1)\n",
    "    \n",
    "    # Transform to world coordinates\n",
    "    points_cam_hom = np.concatenate([points_cam, np.ones((len(points_cam), 1))], axis=1)\n",
    "    camera_pose_inv = np.linalg.inv(camera_pose)\n",
    "    points_world_hom = (camera_pose_inv @ points_cam_hom.T).T\n",
    "    points_world = points_world_hom[:, :3]\n",
    "    \n",
    "    # Get colors if RGB image provided\n",
    "    colors = None\n",
    "    if rgb_image is not None:\n",
    "        colors = rgb_image[v_coords, u_coords]\n",
    "        if colors.dtype != np.uint8:\n",
    "            colors = (colors * 255).astype(np.uint8)\n",
    "    \n",
    "    return {\n",
    "        'points': points_world,\n",
    "        'colors': colors,\n",
    "        'num_points': len(points_world)\n",
    "    }\n",
    "\n",
    "def compute_pointcloud_iou(pc1: np.ndarray, pc2: np.ndarray, voxel_size: float = 0.05) -> float:\n",
    "    \"\"\"Compute IoU between two point clouds using voxelization.\"\"\"\n",
    "    # Check for empty point clouds\n",
    "    if len(pc1) == 0 or len(pc2) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Voxelize both point clouds (PROVIDED)\n",
    "    def voxelize(points, voxel_size):\n",
    "        \"\"\"Convert points to voxel grid indices.\"\"\"\n",
    "        voxel_indices = np.floor(points / voxel_size).astype(int)\n",
    "        return set(map(tuple, voxel_indices))\n",
    "    \n",
    "    voxels1 = voxelize(pc1, voxel_size)\n",
    "    voxels2 = voxelize(pc2, voxel_size)\n",
    "    \n",
    "    # ==================== STUDENT TASK 2 ====================\n",
    "    # TODO: Calculate Intersection over Union (IoU)\n",
    "    #\n",
    "    # IoU = (number of shared voxels) / (total unique voxels)\n",
    "    #\n",
    "    # Your task:\n",
    "    # 1. Count voxels that appear in BOTH sets (intersection)\n",
    "    # 2. Count voxels that appear in EITHER set (union)  \n",
    "    # 3. Handle edge case where union is 0\n",
    "    # 4. Return IoU ratio\n",
    "    #\n",
    "    # Hint: You can use Python set operations:\n",
    "    # - & for intersection (elements in both sets)\n",
    "    # - | for union (elements in either set)\n",
    "    # - len() to count elements\n",
    "    \n",
    "    # TODO: Calculate intersection and union (2 lines)\n",
    "    intersection = 0  # TODO: Replace with actual intersection count\n",
    "    union = 1        # TODO: Replace with actual union count (avoid div by 0)\n",
    "    \n",
    "    # Placeholder - replace with your implementation\n",
    "    return 0.0\n",
    "    # ==================== END STUDENT TASK 2 ====================\n",
    "\n",
    "def fuse_object_pointclouds(pointcloud_observations: List[Dict],\n",
    "                            overlap_threshold: float = 0.1,\n",
    "                            min_observations: int = 2) -> List[Dict]:\n",
    "    \"\"\"Fuse overlapping point clouds of same class.\"\"\"\n",
    "    if not pointcloud_observations:\n",
    "        return []\n",
    "    \n",
    "    print(f\"Fusing {len(pointcloud_observations)} observations with threshold {overlap_threshold}\")\n",
    "    \n",
    "    # Group by class\n",
    "    observations_by_class = {}\n",
    "    for obs in pointcloud_observations:\n",
    "        class_name = obs['label']\n",
    "        if class_name not in observations_by_class:\n",
    "            observations_by_class[class_name] = []\n",
    "        observations_by_class[class_name].append(obs)\n",
    "    \n",
    "    fused_objects = []\n",
    "    \n",
    "    for class_name, class_observations in observations_by_class.items():\n",
    "        if len(class_observations) < min_observations:\n",
    "            continue\n",
    "        \n",
    "        # Build adjacency graph: which observations overlap?\n",
    "        n = len(class_observations)\n",
    "        overlaps = np.zeros((n, n))\n",
    "        \n",
    "        # Compute pairwise overlaps\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                pc1 = class_observations[i]['points']\n",
    "                pc2 = class_observations[j]['points']\n",
    "                \n",
    "                iou = compute_pointcloud_iou(pc1, pc2, voxel_size=0.1)\n",
    "                overlaps[i, j] = iou\n",
    "                overlaps[j, i] = iou\n",
    "        \n",
    "        # Find connected components using graph traversal\n",
    "        visited = [False] * n\n",
    "        clusters = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            if visited[i]:\n",
    "                continue\n",
    "            \n",
    "            # Start new cluster with DFS\n",
    "            cluster = []\n",
    "            stack = [i]\n",
    "            \n",
    "            while stack:\n",
    "                current = stack.pop()\n",
    "                if visited[current]:\n",
    "                    continue\n",
    "                \n",
    "                visited[current] = True\n",
    "                cluster.append(current)\n",
    "                \n",
    "                # Add neighbors with sufficient overlap\n",
    "                for j in range(n):\n",
    "                    if not visited[j] and overlaps[current, j] >= overlap_threshold:\n",
    "                        stack.append(j)\n",
    "            \n",
    "            clusters.append(cluster)\n",
    "        \n",
    "        # Create fused objects from clusters\n",
    "        for cluster in clusters:\n",
    "            if len(cluster) < min_observations:\n",
    "                continue\n",
    "            \n",
    "            # Merge all points in cluster\n",
    "            all_points = []\n",
    "            all_colors = []\n",
    "            all_scores = []\n",
    "            \n",
    "            for obs_idx in cluster:\n",
    "                obs = class_observations[obs_idx]\n",
    "                all_points.append(obs['points'])\n",
    "                if obs.get('colors') is not None:\n",
    "                    all_colors.append(obs['colors'])\n",
    "                all_scores.append(obs['score'])\n",
    "            \n",
    "            # Concatenate\n",
    "            fused_points = np.vstack(all_points)\n",
    "            fused_colors = np.vstack(all_colors) if all_colors else None\n",
    "            \n",
    "            # Downsample if too many points\n",
    "            if len(fused_points) > 5000:\n",
    "                indices = np.random.choice(len(fused_points), 5000, replace=False)\n",
    "                fused_points = fused_points[indices]\n",
    "                if fused_colors is not None:\n",
    "                    fused_colors = fused_colors[indices]\n",
    "            \n",
    "            fused_object = {\n",
    "                'label': class_name,\n",
    "                'points': fused_points,\n",
    "                'colors': fused_colors,\n",
    "                'num_observations': len(cluster),\n",
    "                'avg_score': float(np.mean(all_scores)),\n",
    "                'cluster_id': f\"{class_name}_{len(fused_objects)}\"\n",
    "            }\n",
    "            \n",
    "            fused_objects.append(fused_object)\n",
    "    \n",
    "    print(f\"Fused into {len(fused_objects)} objects\")\n",
    "    return fused_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf54006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bbox_from_pointcloud(points: np.ndarray,\n",
    "                                  method: str = 'aabb',\n",
    "                                  outlier_ratio: float = 0.05) -> Dict:\n",
    "    \"\"\"Generate 3D bounding box from point cloud.\"\"\"\n",
    "    if len(points) < 10:\n",
    "        return None\n",
    "    \n",
    "    # Remove outliers using statistical filtering\n",
    "    if outlier_ratio > 0:\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(points)\n",
    "        \n",
    "        # Statistical outlier removal\n",
    "        pcd_filtered, _ = pcd.remove_statistical_outlier(\n",
    "            nb_neighbors=20,\n",
    "            std_ratio=2.0\n",
    "        )\n",
    "        \n",
    "        if len(pcd_filtered.points) < 10:\n",
    "            filtered_points = points\n",
    "        else:\n",
    "            filtered_points = np.asarray(pcd_filtered.points)\n",
    "    else:\n",
    "        filtered_points = points\n",
    "    \n",
    "    if method == 'obb':\n",
    "        # Create Open3D point cloud\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(filtered_points)\n",
    "        \n",
    "        # Compute oriented bounding box\n",
    "        try:\n",
    "            obb = pcd.get_oriented_bounding_box()\n",
    "            \n",
    "            # Get OBB parameters\n",
    "            center = np.asarray(obb.center)\n",
    "            extent = np.asarray(obb.extent)\n",
    "            rotation = np.asarray(obb.R)\n",
    "            \n",
    "            # Get 8 corners\n",
    "            corners = np.asarray(obb.get_box_points())\n",
    "            \n",
    "            return {\n",
    "                'center': center,\n",
    "                'extent': extent,\n",
    "                'rotation': rotation,\n",
    "                'corners': corners,\n",
    "                'type': 'obb'\n",
    "            }\n",
    "        except:\n",
    "            # Fall back to AABB if OBB fails\n",
    "            method = 'aabb'\n",
    "    \n",
    "    if method == 'aabb':\n",
    "        # Compute axis-aligned bounding box\n",
    "        min_bounds = np.min(filtered_points, axis=0)\n",
    "        max_bounds = np.max(filtered_points, axis=0)\n",
    "        \n",
    "        center = (min_bounds + max_bounds) / 2\n",
    "        extent = max_bounds - min_bounds\n",
    "        \n",
    "        # Generate 8 corners\n",
    "        corners = np.array([\n",
    "            [min_bounds[0], min_bounds[1], min_bounds[2]],\n",
    "            [max_bounds[0], min_bounds[1], min_bounds[2]],\n",
    "            [min_bounds[0], max_bounds[1], min_bounds[2]],\n",
    "            [max_bounds[0], max_bounds[1], min_bounds[2]],\n",
    "            [min_bounds[0], min_bounds[1], max_bounds[2]],\n",
    "            [max_bounds[0], min_bounds[1], max_bounds[2]],\n",
    "            [min_bounds[0], max_bounds[1], max_bounds[2]],\n",
    "            [max_bounds[0], max_bounds[1], max_bounds[2]],\n",
    "        ])\n",
    "        \n",
    "        return {\n",
    "            'center': center,\n",
    "            'extent': extent,\n",
    "            'rotation': np.eye(3),\n",
    "            'corners': corners,\n",
    "            'type': 'aabb'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebbd4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame_with_sam(frame_data: Dict,\n",
    "                          owl_processor, owl_model,\n",
    "                          sam_processor, sam_model,\n",
    "                          device: str,\n",
    "                          config: Config) -> Dict:\n",
    "    \"\"\"Process a single frame with OwlV2 detection and SAM segmentation.\"\"\"\n",
    "    results = {\n",
    "        'frame_name': frame_data['frame_name'],\n",
    "        'detections_2d': [],\n",
    "        'segments': [],\n",
    "        'pointclouds': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Run OwlV2 detection\n",
    "        detections_2d, image = detect_objects_in_frame(\n",
    "            frame_data['rgb_path'],\n",
    "            config.OBJECT_CLASSES,\n",
    "            owl_processor,\n",
    "            owl_model,\n",
    "            device,\n",
    "            threshold=config.LEVEL_C_CONFIG['detection_threshold']\n",
    "        )\n",
    "        \n",
    "        if not detections_2d:\n",
    "            return results\n",
    "        \n",
    "        results['detections_2d'] = detections_2d\n",
    "        \n",
    "        # Load depth and RGB images\n",
    "        depth_image = cv2.imread(frame_data['depth_path'], cv2.IMREAD_UNCHANGED)\n",
    "        rgb_image = np.array(image)\n",
    "        \n",
    "        # Process each detection with SAM\n",
    "        for detection in detections_2d:\n",
    "            # Generate segmentation mask using SAM\n",
    "            mask = segment_with_sam_bbox(\n",
    "                image,\n",
    "                detection['bbox'],\n",
    "                sam_model,\n",
    "                sam_processor,\n",
    "                device,\n",
    "                confidence_threshold=config.LEVEL_C_CONFIG['sam_confidence_threshold']\n",
    "            )\n",
    "            \n",
    "            if mask is None:\n",
    "                continue\n",
    "            \n",
    "            # Convert segment to 3D point cloud\n",
    "            pc_data = segment_to_3d_pointcloud(\n",
    "                mask,\n",
    "                depth_image,\n",
    "                rgb_image,\n",
    "                frame_data['camera_intrinsics'],\n",
    "                frame_data['camera_pose'],\n",
    "                depth_scale=config.TSDF_CONFIG['depth_scale'],\n",
    "                max_points=config.LEVEL_C_CONFIG.get('max_points_per_object', 2000)\n",
    "            )\n",
    "            \n",
    "            if pc_data is None or pc_data['num_points'] < 10:\n",
    "                continue\n",
    "            \n",
    "            # Store results\n",
    "            results['segments'].append({\n",
    "                'bbox': detection['bbox'],\n",
    "                'label': detection['label'],\n",
    "                'score': detection['score'],\n",
    "                'mask': mask\n",
    "            })\n",
    "            \n",
    "            results['pointclouds'].append({\n",
    "                'label': detection['label'],\n",
    "                'score': detection['score'],\n",
    "                'points': pc_data['points'],\n",
    "                'colors': pc_data['colors'],\n",
    "                'frame_name': frame_data['frame_name']\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing frame {frame_data['frame_name']}: {e}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe926ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_detections(fused_objects: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Generate final detection results from fused point cloud objects.\"\"\"\n",
    "    final_detections = []\n",
    "    \n",
    "    for fused_obj in fused_objects:\n",
    "        bbox_data = generate_bbox_from_pointcloud(\n",
    "            fused_obj['points'],\n",
    "            method='aabb',\n",
    "            outlier_ratio=0.15\n",
    "        )\n",
    "        \n",
    "        if bbox_data is None:\n",
    "            continue\n",
    "        \n",
    "        final_detections.append({\n",
    "            'label': fused_obj['label'],\n",
    "            'score': fused_obj['avg_score'],\n",
    "            'center_3d_world': bbox_data['center'].tolist(),\n",
    "            'bbox_3d_world': bbox_data['corners'].tolist(),\n",
    "            'extent': bbox_data['extent'].tolist(),\n",
    "            'rotation': bbox_data['rotation'].tolist(),\n",
    "            'bbox_type': bbox_data['type'],\n",
    "            'num_observations': fused_obj['num_observations'],\n",
    "            'pointcloud': fused_obj['points'],\n",
    "            'colors': fused_obj['colors']\n",
    "        })\n",
    "    \n",
    "    return final_detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b73eb2",
   "metadata": {},
   "source": [
    "## 7. Full Pipeline Execution\n",
    "\n",
    "Now let's run the complete SAM-enhanced pipeline across all frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde1dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the complete SAM-enhanced 3D object detection pipeline\n",
    "print(\"Running full SAM-enhanced pipeline across all frames...\")\n",
    "\n",
    "pipeline_results = run_full_pipeline(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
