{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a536cf2",
   "metadata": {},
   "source": [
    "# Level E instructions:\n",
    "\n",
    "### Some steps to get familiar with the data:\n",
    "\n",
    "- Have a look at the rgb images in the folder \"ARKitScenesData/47333473/47333473_frames/lowres_wide/\" to get an impression of the scene that we are working with\n",
    "- Run the notebook to see the complete pipeline: ground truth visualization, example detection, and full results\n",
    "\n",
    "## Level E\n",
    "\n",
    "The task is to generate 3D bounding boxes that mark the estimated location of objects in the environment.\n",
    "\n",
    "Required to pass the level:\n",
    "- Functional pipeline with visualization of the estimated 3D bounding boxes\n",
    "- mIOU score > 0.15 of the bounding box estimates when comparing to ground-truth (For classes: \"bed\", \"sofa\", \"chair\", \"table\", \"shelf\")\n",
    "- Understand and explain the code flow and steps required for the complete pipeline\n",
    "- Show at least one detection of a non-ground-truth object (see last cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f80226",
   "metadata": {},
   "source": [
    "## 1. Dependencies and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install torch==2.4.0+cu121 torchvision==0.19.0+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers==4.44.0 huggingface-hub==0.24.0 pillow numpy opencv-python open3d ipympl rerun-sdk[notebook]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191d71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Import lab utility functions\n",
    "from lab_utils.data_utils import get_frame_list, load_camera_poses, validate_and_align_frame_data\n",
    "from lab_utils.ground_truth import load_ground_truth_data\n",
    "from lab_utils.tsdf_utils import build_tsdf_point_cloud\n",
    "from lab_utils.viz_eval import visualize_3d_scene_bbox_results, evaluate_level_results\n",
    "from lab_utils.model_loaders import load_owlv2_model\n",
    "from lab_utils.level_specific_viz import visualize_level_e_example\n",
    "from lab_utils.batch_processing_utils import process_frames_in_batches\n",
    "from lab_utils.detection_utils import (\n",
    "    detect_objects_in_frame, generate_3d_detections, merge_overlapping_detections, compute_3d_bbox_iou\n",
    ")\n",
    "from lab_utils.visualization_utils import visualize_2d_detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db590fb7",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Scene Configuration\n",
    "    SCENE_ID = \"47333473\"\n",
    "    BASE_PATH = f\"ARKitScenesData/{SCENE_ID}/{SCENE_ID}_frames\"\n",
    "    RGB_PATH = os.path.join(BASE_PATH, \"lowres_wide\")\n",
    "    DEPTH_PATH = os.path.join(BASE_PATH, \"lowres_depth\")\n",
    "    INTRINSICS_PATH = os.path.join(BASE_PATH, \"lowres_wide_intrinsics\")\n",
    "    TRAJ_FILE_PATH = os.path.join(BASE_PATH, \"lowres_wide.traj\")\n",
    "    \n",
    "    # Detection Classes\n",
    "    OBJECT_CLASSES = [\"bed\", \"chair\", \"sofa\", \"table\", \"shelf\"]\n",
    "    \n",
    "    # Level E Configuration\n",
    "    LEVEL_E_CONFIG = {\n",
    "        'frame_skip': 3,\n",
    "        'max_frames': 1000,\n",
    "        'detection_threshold': 0.5,\n",
    "        'owl_batch_size': 20,\n",
    "        'merge_iou_threshold': 0.3,\n",
    "        'min_detections_for_merge': 4,\n",
    "        'example_viz_index': 47,\n",
    "        'required_miou': 0.15\n",
    "    }\n",
    "    \n",
    "    # TSDF Configuration\n",
    "    TSDF_CONFIG = {\n",
    "        'frame_skip': 3, 'depth_scale': 1000.0, 'depth_trunc': 7.0,\n",
    "        'voxel_size': 0.04, 'batch_size': 20, 'max_frames': 1000,\n",
    "        'volume_length': 30.0, 'resolution': 512,\n",
    "    }\n",
    "\n",
    "    # GT and Display Configuration\n",
    "    GT_CONFIG = {\n",
    "        'allowed_classes': None, 'mesh_downsample_points': 75000,\n",
    "        'show_mesh': True, 'show_annotations': True\n",
    "    }\n",
    "    RERUN_WIDTH, RERUN_HEIGHT = 1200, 500\n",
    "\n",
    "def validate_config(config: Config) -> None:\n",
    "    \"\"\"Validate and initialize configuration parameters.\"\"\"\n",
    "    config.GT_CONFIG['allowed_classes'] = config.OBJECT_CLASSES\n",
    "    print(f\"Configuration validated.\")\n",
    "\n",
    "# Create and validate config\n",
    "config = Config()\n",
    "validate_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uattql8o",
   "metadata": {},
   "source": [
    "## 3. Scene Understanding - Ground Truth Visualization\n",
    "\n",
    "Let's start by visualizing the ground truth data to understand what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molf93yo8m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize ground truth to understand the scene\n",
    "print(\"Loading ground truth data to understand our scene...\")\n",
    "\n",
    "gt_annotations, gt_mesh = load_ground_truth_data(\n",
    "    config.SCENE_ID, \n",
    "    config.BASE_PATH,\n",
    "    config.GT_CONFIG\n",
    ")\n",
    "\n",
    "if gt_annotations:\n",
    "    print(f\"âœ“ Loaded {len(gt_annotations)} ground truth annotations\")\n",
    "    class_counts = {}\n",
    "    for ann in gt_annotations:\n",
    "        class_counts[ann['label']] = class_counts.get(ann['label'], 0) + 1\n",
    "    print(f\"Objects in scene: {class_counts}\")\n",
    "\n",
    "if gt_mesh:\n",
    "    print(f\"âœ“ Loaded ground truth mesh with {len(gt_mesh.points)} points\")\n",
    "\n",
    "# Visualize the ground truth scene\n",
    "visualize_3d_scene_bbox_results(\n",
    "    point_cloud=None,\n",
    "    detections_3d=None,\n",
    "    gt_annotations=gt_annotations,\n",
    "    gt_mesh=gt_mesh,\n",
    "    show_ground_truth=True,\n",
    "    show_gt_mesh=True,\n",
    "    show_object_pointclouds=False,\n",
    "    title=f\"Ground Truth Scene {config.SCENE_ID} - What We Want to Detect\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"ðŸ‘† This shows the ground truth objects we want to detect!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae270d",
   "metadata": {},
   "source": [
    "## 4. Execution Functions\n",
    "\n",
    "These functions orchestrate the different parts of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ground_truth_visualization(config: Config) -> None:\n",
    "    \"\"\"Execute ground truth visualization.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"GROUND TRUTH VISUALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    gt_annotations, gt_mesh = load_ground_truth_data(\n",
    "        config.SCENE_ID, \n",
    "        config.BASE_PATH,\n",
    "        config.GT_CONFIG\n",
    "    )\n",
    "    \n",
    "    if gt_annotations:\n",
    "        print(f\"Loaded {len(gt_annotations)} ground truth annotations\")\n",
    "        class_counts = {}\n",
    "        for ann in gt_annotations:\n",
    "            class_counts[ann['label']] = class_counts.get(ann['label'], 0) + 1\n",
    "        print(f\"GT objects by class: {class_counts}\")\n",
    "    \n",
    "    if gt_mesh:\n",
    "        print(f\"Loaded ground truth mesh with {len(gt_mesh.points)} points\")\n",
    "    \n",
    "    visualize_3d_scene_bbox_results(\n",
    "        point_cloud=None,\n",
    "        detections_3d=None,\n",
    "        gt_annotations=gt_annotations,\n",
    "        gt_mesh=gt_mesh,\n",
    "        show_ground_truth=True,\n",
    "        show_gt_mesh=True,\n",
    "        show_object_pointclouds=False,\n",
    "        title=f\"Ground Truth Only - Scene {config.SCENE_ID}\",\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    print(\"Ground truth visualization complete!\")\n",
    "\n",
    "\n",
    "def run_example_visualization(config: Config) -> Dict:\n",
    "    \"\"\"Execute example visualization.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXAMPLE VISUALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    example_results = visualize_level_e_example(\n",
    "        config, \n",
    "        frame_index=config.LEVEL_E_CONFIG['example_viz_index'],\n",
    "        show_depth_analysis=True\n",
    "    )\n",
    "    \n",
    "    print(\"Example visualization complete!\")\n",
    "    return example_results\n",
    "\n",
    "\n",
    "def run_full_pipeline(config: Config) -> Dict:\n",
    "    \"\"\"Execute the complete 3D scene analysis pipeline.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FULL PIPELINE EXECUTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    processor, model, device = load_owlv2_model()\n",
    "    camera_poses = load_camera_poses(config.TRAJ_FILE_PATH)\n",
    "    frames_metadata = get_frame_list(config.RGB_PATH, config.LEVEL_E_CONFIG['frame_skip'])\n",
    "    aligned_frames = validate_and_align_frame_data(\n",
    "        frames_metadata, camera_poses, config.RGB_PATH, \n",
    "        config.DEPTH_PATH, config.INTRINSICS_PATH, timestamp_tolerance=0.1\n",
    "    )\n",
    "    \n",
    "    if not aligned_frames:\n",
    "        print(\"ERROR: No aligned frames found! Check data paths.\")\n",
    "        return {'detections_3d': [], 'frame_results': {}, 'statistics': {}}\n",
    "    \n",
    "    frames_for_detection = aligned_frames[:config.LEVEL_E_CONFIG['max_frames']]\n",
    "    raw_detections_3d, frame_results, detection_stats = process_frames_in_batches(\n",
    "        frames_for_detection, config, processor, model, device\n",
    "    )\n",
    "    \n",
    "    merged_detections = merge_overlapping_detections(\n",
    "        raw_detections_3d,\n",
    "        iou_threshold=config.LEVEL_E_CONFIG['merge_iou_threshold'],\n",
    "        min_detections_for_merge=config.LEVEL_E_CONFIG['min_detections_for_merge']\n",
    "    ) if raw_detections_3d else []\n",
    "    \n",
    "    print(f\"Pipeline: {len(frames_for_detection)} frames â†’ {len(raw_detections_3d)} raw â†’ {len(merged_detections)} merged\")\n",
    "    \n",
    "    tsdf_point_cloud = build_tsdf_point_cloud(config, max_frames_for_mapping=596, use_cached=True)\n",
    "    gt_annotations, gt_mesh = load_ground_truth_data(config.SCENE_ID, config.BASE_PATH, config.GT_CONFIG)\n",
    "    eval_results = evaluate_level_results(merged_detections, gt_annotations, \"Level E (OwlV2 Only)\", required_miou=config.LEVEL_E_CONFIG['required_miou'])\n",
    "    \n",
    "    print(f\"Results: {'âœ“ PASSED' if eval_results['passed'] else 'âœ— FAILED'} | \"\n",
    "          f\"mIoU: {eval_results['mean_iou']:.3f} | Detections: {eval_results['num_detections']}\")\n",
    "    \n",
    "    if merged_detections or raw_detections_3d or tsdf_point_cloud:\n",
    "        visualize_3d_scene_bbox_results(\n",
    "            point_cloud=tsdf_point_cloud, detections_3d=merged_detections,\n",
    "            raw_detections_3d=raw_detections_3d, gt_annotations=gt_annotations,\n",
    "            gt_mesh=None, show_ground_truth=True, show_gt_mesh=False,\n",
    "            show_object_pointclouds=False, show_raw_detections=True,\n",
    "            title=f\"Level E: Raw + Merged Detections - Scene {config.SCENE_ID}\",\n",
    "            config=config\n",
    "        )\n",
    "    \n",
    "    print(\"Full pipeline complete!\")\n",
    "    return {\n",
    "        'detections_3d': merged_detections,\n",
    "        'raw_detections_3d': raw_detections_3d,\n",
    "        'frame_results': frame_results,\n",
    "        'statistics': {\n",
    "            **detection_stats,\n",
    "            'total_3d_detections_merged': len(merged_detections),\n",
    "            'detection_classes': list(detection_stats['detection_classes']),\n",
    "            'alignment_success_rate': len(aligned_frames) / len(frames_metadata) * 100 if frames_metadata else 0,\n",
    "            'merge_ratio': len(merged_detections) / len(raw_detections_3d) if raw_detections_3d else 0\n",
    "        },\n",
    "        'evaluation': eval_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdbaedf",
   "metadata": {},
   "source": [
    "## 5. 2D Detection with OwlV2 Model\n",
    "\n",
    "Complete the TODOs in the functions below and run the visualization to understand the object detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851fbbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_owlv2_outputs(outputs, processor, target_sizes, text_queries, threshold=0.1):\n",
    "    \"\"\"Process raw OwlV2 model outputs into detection format.\"\"\"\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs=outputs, target_sizes=target_sizes, threshold=threshold\n",
    "    )[0]\n",
    "    \n",
    "    # TODO: Extract results, move to cpu and convert to numpy arrays\n",
    "    # boxes = # YOUR CODE HERE\n",
    "    # scores = # YOUR CODE HERE\n",
    "    # labels = # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Create detection list\n",
    "    detections = []\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        detections.append({\n",
    "            'bbox': box.tolist(),\n",
    "            'score': float(score), \n",
    "            'label': text_queries[label],\n",
    "            'label_id': int(label)\n",
    "        })\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224b74d2",
   "metadata": {},
   "source": [
    "Visualize the detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4lm9vj55rud",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run example visualization to see detection process on a single frame\n",
    "print(\"Running example detection on a single frame...\")\n",
    "\n",
    "example_results = visualize_level_e_example(\n",
    "    config, \n",
    "    frame_index=config.LEVEL_E_CONFIG['example_viz_index'],\n",
    "    show_depth_analysis=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6942e",
   "metadata": {},
   "source": [
    "## 6. 3D Processing Implementation\n",
    "Implement the TODOs below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff469319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Camera Projection\n",
    "def project_pixel_to_3d(center_pixel: List[float], depth: float, camera_intrinsics: np.ndarray) -> List[float]:\n",
    "    \"\"\"Project a single pixel + depth to 3D coordinates.\"\"\"\n",
    "    # TODO: Extract camera parameters and apply pinhole model\n",
    "    # fx, fy = camera_intrinsics[0,0], camera_intrinsics[1,1]  \n",
    "    # cx, cy = camera_intrinsics[0,2], camera_intrinsics[1,2]\n",
    "    # center_u, center_v = center_pixel\n",
    "    # x_3d = (center_u - cx) * depth / fx  # YOUR CODE HERE\n",
    "    # y_3d = # YOUR CODE HERE\n",
    "    # z_3d = # YOUR CODE HERE\n",
    "    \n",
    "    # Dummy placeholders (replace with your implementation above)\n",
    "    x_3d, y_3d, z_3d = 0.0, 0.0, depth\n",
    "    \n",
    "    return [float(x_3d), float(y_3d), float(z_3d)]\n",
    "\n",
    "# TODO 2: Find Overlapping Detections (mostly provided, fill in the TODO parts)\n",
    "def find_overlapping_detections(detections_3d: List[Dict], class_name: str, iou_threshold: float = 0.4) -> List[List[int]]:\n",
    "    \"\"\"Find which detections of same class overlap significantly.\"\"\"\n",
    "    # Filter by class\n",
    "    class_detections = [i for i, d in enumerate(detections_3d) if d['label'] == class_name]\n",
    "    \n",
    "    overlapping_pairs = []\n",
    "    \n",
    "    # Check all pairs\n",
    "    for i in range(len(class_detections)):\n",
    "        for j in range(i + 1, len(class_detections)):\n",
    "            idx1, idx2 = class_detections[i], class_detections[j]\n",
    "            \n",
    "            # TODO: Compute IoU between the two 3D bounding boxes (hint: check if you can use a function defined above)\n",
    "            iou = 0.0  # Replace this line with your code\n",
    "            \n",
    "            # TODO: Check if IoU exceeds threshold and append to overlapping_pairs\n",
    "            # if ???:\n",
    "            #     overlapping_pairs.append([idx1, idx2])\n",
    "    \n",
    "    return overlapping_pairs  # Returns list of overlapping pairs of indices\n",
    "\n",
    "# PROVIDED: Merge Detection Cluster\n",
    "def merge_detection_cluster(detection_indices: List[int], all_detections: List[Dict]) -> Dict:\n",
    "    \"\"\"Merge a cluster of overlapping detections into one.\"\"\"\n",
    "    # Extract detections\n",
    "    cluster_detections = [all_detections[i] for i in detection_indices]\n",
    "    \n",
    "    # Get all corners and scores\n",
    "    all_corners = [det['bbox_3d_world'] for det in cluster_detections]\n",
    "    all_scores = [det['score'] for det in cluster_detections]\n",
    "    \n",
    "    # Average the corners and scores\n",
    "    merged_corners = np.mean(all_corners, axis=0)\n",
    "    avg_score = np.mean(all_scores)\n",
    "    \n",
    "    # Get class name\n",
    "    class_name = cluster_detections[0]['label']\n",
    "    \n",
    "    # Calculate center from averaged corners\n",
    "    corners_array = np.array(merged_corners)\n",
    "    center = np.mean(corners_array, axis=0)\n",
    "    \n",
    "    return {\n",
    "        'label': class_name,\n",
    "        'score': float(avg_score),\n",
    "        'bbox_3d_world': merged_corners.tolist(),\n",
    "        'center_3d_world': center.tolist(),\n",
    "        'merge_info': {\n",
    "            'num_detections_merged': len(detection_indices),\n",
    "            'original_confidences': all_scores\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1577e718",
   "metadata": {},
   "source": [
    "## 7. Full Pipeline Execution\n",
    "\n",
    "Now let's run the complete pipeline across all frames. You can enable and disable visualization of different things (GT, raw detections, merged, ...) in the viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c96bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the complete 3D object detection pipeline\n",
    "print(\"Running full pipeline across all frames...\")\n",
    "\n",
    "pipeline_results = run_full_pipeline(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6194de",
   "metadata": {},
   "source": [
    "## 8. Open Vocabulary Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2198f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First complete the main pipeline above, then set RUN_EXPLORATION = True to experiment\n",
    "\n",
    "RUN_EXPLORATION = False  # TODO: Set to True when ready to explore\n",
    "\n",
    "EXPLORATION_CLASSES = [\n",
    "    \"bed\", \"chair\", \"sofa\", \"table\", \"shelf\",  # Keep original classes\n",
    "    # TODO: Add 2-3 additional objects you want to find:\n",
    "    # \"obj1\",\n",
    "    # \"obj2\"\n",
    "]\n",
    "\n",
    "if RUN_EXPLORATION:\n",
    "    if len(EXPLORATION_CLASSES) > 5:\n",
    "        print(f\"Exploring: {EXPLORATION_CLASSES}\")\n",
    "        \n",
    "        # Create new config with exploration classes\n",
    "        exploration_config = Config()\n",
    "        exploration_config.OBJECT_CLASSES = EXPLORATION_CLASSES\n",
    "        \n",
    "        # Run pipeline\n",
    "        results = run_full_pipeline(exploration_config)\n",
    "        \n",
    "        # Show what was found\n",
    "        detections = results.get('detections_3d', [])\n",
    "        print(f\"Found {len(detections)} objects: {[d['label'] for d in detections]}\")\n",
    "    else:\n",
    "        print(\"Add some new objects to EXPLORATION_CLASSES to start exploring!\")\n",
    "else:\n",
    "    print(\"Set RUN_EXPLORATION = True to try detecting different objects\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
